{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torchshow as ts\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importere dataset, kun træning indtil videre.\n",
    "training_images = MNIST(root='data', transform=ToTensor(), train=True)\n",
    "training_dataloader = DataLoader(training_images, batch_size=1000)\n",
    "testing_images = MNIST(root='data', transform=ToTensor(), train=False)\n",
    "testing_dataloader = DataLoader(testing_images, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laver det Neurale Netværk, og opstiller et accuracy objekt til at måle hvor god modellen er.\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, kernel_size=3), #første parameter 1 er antal kanaler, her 1 fordi vi arbejder med gråtoner; 12 er antal ouputkanaler, altså antal filtre; 3 er størrelsen på det udsnit af billedet vi tager, som så bliver 3x3 matrice af pixels.\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2), #vælger den maksimale værdi i et udsnit af størrelsen 2x2, således dimensionerne af dataene reduceres fra 28x28 til 14x14\n",
    "    nn.Conv2d(10, 10, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2), #vælger den maksimale værdi i et udsnit af størrelsen 2x2, således dimensionerne af dataene reduceres fra 14x14 til 7x7\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(250,10), # input er nu 5 x 5 x 10\n",
    ")\n",
    "# Bruger crossentropy til at udregne losset, og indstiller optimizeren.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop'er over 10 epoker, og udregner loss'et og accuracy for hvert.\n",
    "def training_loop(training_dataloader, optimizer, loss_fn):\n",
    "    total_loss = 0\n",
    "    accuracy.reset()\n",
    "    size = len(training_dataloader)\n",
    "    for images,labels in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy.update(output, labels)\n",
    "    avg_loss = total_loss / size\n",
    "    print(f\"Avg Training Accuracy: {accuracy.compute() * 100:.2f}%\")\n",
    "    print(f\"Avg Training Loss: {avg_loss}\")\n",
    "\n",
    "def testing_loop(testing_dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    accuracy.reset()\n",
    "    size = len(testing_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for images,labels in testing_dataloader:\n",
    "            output = net(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            total_loss += loss\n",
    "            accuracy.update(output,labels)\n",
    "    avg_loss = total_loss / size\n",
    "    print(f\"Avg Testing Accuracy: {accuracy.compute() * 100 :.2f}%\")\n",
    "    print(f\"Avg Testing Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Avg Testing Accuracy: 9.02%\n",
      "Avg Testing Loss: 2.300119400024414\n",
      "Avg Training Accuracy: 85.55%\n",
      "Avg Training Loss: 0.5155499577522278\n",
      "------------------\n",
      "Epoch: 1\n",
      "Avg Testing Accuracy: 95.85%\n",
      "Avg Testing Loss: 0.14929404854774475\n",
      "Avg Training Accuracy: 96.24%\n",
      "Avg Training Loss: 0.12710736691951752\n",
      "------------------\n",
      "Epoch: 2\n",
      "Avg Testing Accuracy: 97.29%\n",
      "Avg Testing Loss: 0.09350116550922394\n",
      "Avg Training Accuracy: 97.34%\n",
      "Avg Training Loss: 0.08821598440408707\n",
      "------------------\n",
      "Epoch: 3\n",
      "Avg Testing Accuracy: 97.65%\n",
      "Avg Testing Loss: 0.0727459192276001\n",
      "Avg Training Accuracy: 97.84%\n",
      "Avg Training Loss: 0.07086966186761856\n",
      "------------------\n",
      "Epoch: 4\n",
      "Avg Testing Accuracy: 97.85%\n",
      "Avg Testing Loss: 0.07019895315170288\n",
      "Avg Training Accuracy: 98.18%\n",
      "Avg Training Loss: 0.06120595708489418\n",
      "------------------\n",
      "Epoch: 5\n",
      "Avg Testing Accuracy: 97.99%\n",
      "Avg Testing Loss: 0.06644191592931747\n",
      "Avg Training Accuracy: 98.34%\n",
      "Avg Training Loss: 0.05475080758333206\n",
      "------------------\n",
      "Epoch: 6\n",
      "Avg Testing Accuracy: 97.97%\n",
      "Avg Testing Loss: 0.06132178753614426\n",
      "Avg Training Accuracy: 98.49%\n",
      "Avg Training Loss: 0.05030346289277077\n",
      "------------------\n",
      "Epoch: 7\n",
      "Avg Testing Accuracy: 98.22%\n",
      "Avg Testing Loss: 0.05295976996421814\n",
      "Avg Training Accuracy: 98.58%\n",
      "Avg Training Loss: 0.04666627570986748\n",
      "------------------\n",
      "Epoch: 8\n",
      "Avg Testing Accuracy: 98.35%\n",
      "Avg Testing Loss: 0.0501190721988678\n",
      "Avg Training Accuracy: 98.69%\n",
      "Avg Training Loss: 0.04415865242481232\n",
      "------------------\n",
      "Epoch: 9\n",
      "Avg Testing Accuracy: 98.46%\n",
      "Avg Testing Loss: 0.04778818413615227\n",
      "Avg Training Accuracy: 98.75%\n",
      "Avg Training Loss: 0.04202570766210556\n",
      "------------------\n",
      "Epoch: 10\n",
      "Avg Testing Accuracy: 98.45%\n",
      "Avg Testing Loss: 0.04760954901576042\n",
      "Avg Training Accuracy: 98.80%\n",
      "Avg Training Loss: 0.040186621248722076\n",
      "------------------\n",
      "Epoch: 11\n",
      "Avg Testing Accuracy: 98.46%\n",
      "Avg Testing Loss: 0.04967664182186127\n",
      "Avg Training Accuracy: 98.82%\n",
      "Avg Training Loss: 0.038526225835084915\n",
      "------------------\n",
      "Epoch: 12\n",
      "Avg Testing Accuracy: 98.44%\n",
      "Avg Testing Loss: 0.05166206508874893\n",
      "Avg Training Accuracy: 98.87%\n",
      "Avg Training Loss: 0.03649737313389778\n",
      "------------------\n",
      "Epoch: 13\n",
      "Avg Testing Accuracy: 98.45%\n",
      "Avg Testing Loss: 0.049745891243219376\n",
      "Avg Training Accuracy: 98.91%\n",
      "Avg Training Loss: 0.0345718078315258\n",
      "------------------\n",
      "Epoch: 14\n",
      "Avg Testing Accuracy: 98.50%\n",
      "Avg Testing Loss: 0.04957765340805054\n",
      "Avg Training Accuracy: 98.95%\n",
      "Avg Training Loss: 0.03345184400677681\n",
      "------------------\n",
      "Epoch: 15\n",
      "Avg Testing Accuracy: 98.52%\n",
      "Avg Testing Loss: 0.05130895972251892\n",
      "Avg Training Accuracy: 98.99%\n",
      "Avg Training Loss: 0.03285128250718117\n",
      "------------------\n",
      "Epoch: 16\n",
      "Avg Testing Accuracy: 98.39%\n",
      "Avg Testing Loss: 0.05437811464071274\n",
      "Avg Training Accuracy: 98.97%\n",
      "Avg Training Loss: 0.032558832317590714\n",
      "------------------\n",
      "Epoch: 17\n",
      "Avg Testing Accuracy: 98.28%\n",
      "Avg Testing Loss: 0.05718677490949631\n",
      "Avg Training Accuracy: 99.00%\n",
      "Avg Training Loss: 0.03173907473683357\n",
      "------------------\n",
      "Epoch: 18\n",
      "Avg Testing Accuracy: 98.35%\n",
      "Avg Testing Loss: 0.055730246007442474\n",
      "Avg Training Accuracy: 99.03%\n",
      "Avg Training Loss: 0.03046749159693718\n",
      "------------------\n",
      "Epoch: 19\n",
      "Avg Testing Accuracy: 98.45%\n",
      "Avg Testing Loss: 0.05244487524032593\n",
      "Avg Training Accuracy: 99.09%\n",
      "Avg Training Loss: 0.029421012848615646\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"Epoch: {i}\")\n",
    "    testing_loop(testing_dataloader, loss_fn)\n",
    "    training_loop(training_dataloader, optimizer, loss_fn)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "tensor([[ -4.1196, -20.9900,  -4.3859,  -4.8772,  -7.1593,  17.0437,  10.5028,\n",
      "         -13.6950,   2.6679,  -1.0215]], grad_fn=<AddmmBackward0>)\n",
      "Efter softmax:\n",
      "tensor([[6.4309e-10, 3.0306e-17, 4.9273e-10, 3.0149e-10, 3.0772e-11, 9.9856e-01,\n",
      "         1.4411e-03, 4.4640e-14, 5.7022e-07, 1.4249e-08]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Modul bud: 5\n",
      "Korrekt: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afprøver modellen på et givent index af billederne.\n",
    "img_idx = 2000\n",
    "pred = net(training_images[img_idx][0].unsqueeze(0)) \n",
    "print(\"Model output:\")\n",
    "print(pred)\n",
    "sm = nn.Softmax(dim=1)\n",
    "print(\"Efter softmax:\")\n",
    "print(sm(pred))\n",
    "print(f\"Modul bud: {pred.argmax()}\")\n",
    "print(f\"Korrekt: {training_images[img_idx][1]}\")\n",
    "ts.show(training_images[img_idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Documentation og Mikkels ting\n",
    "def preprocess(image):\n",
    "    gs = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "    image = image[0:3]\n",
    "    image = torchvision.transforms.functional.resize(image, [28, 28], antialias=True)\n",
    "    image = gs(image)\n",
    "    image = image.float()/255\n",
    "    image = torchvision.transforms.functional.invert(image)\n",
    "    image = torchvision.transforms.functional.adjust_contrast(image, 13)\n",
    "    tresh = torch.nn.Threshold(0.28, 0.0)\n",
    "    image = tresh(image)\n",
    "    return image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentSystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
