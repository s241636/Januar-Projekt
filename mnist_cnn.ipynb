{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torchshow as ts\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importere dataset, kun træning indtil videre.\n",
    "training_images = MNIST(root='data', transform=ToTensor(), train=True)\n",
    "training_dataloader = DataLoader(training_images, batch_size=1000)\n",
    "testing_images = MNIST(root='data', transform=ToTensor(), train=False)\n",
    "testing_dataloader = DataLoader(testing_images, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laver det Neurale Netværk, og opstiller et accuracy objekt til at måle hvor god modellen er.\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, kernel_size=3), #første parameter 1 er antal kanaler, her 1 fordi vi arbejder med gråtoner; 12 er antal ouputkanaler, altså antal filtre; 3 er størrelsen på det udsnit af billedet vi tager, som så bliver 3x3 matrice af pixels.\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2), #vælger den maksimale værdi i et udsnit af størrelsen 2x2, således dimensionerne af dataene reduceres fra 28x28 til 14x14\n",
    "    nn.Conv2d(10, 10, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2), #vælger den maksimale værdi i et udsnit af størrelsen 2x2, således dimensionerne af dataene reduceres fra 14x14 til 7x7\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(250,10), # input er nu 5 x 5 x 10\n",
    ")\n",
    "# Bruger crossentropy til at udregne losset, og indstiller optimizeren.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop'er over 10 epoker, og udregner loss'et og accuracy for hvert.\n",
    "def training_loop(training_dataloader, optimizer, loss_fn):\n",
    "    total_loss = 0\n",
    "    accuracy.reset()\n",
    "    size = len(training_dataloader)\n",
    "    for images,labels in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy.update(output, labels)\n",
    "    avg_loss = total_loss / size\n",
    "    print(f\"Avg Training Accuracy: {accuracy.compute() * 100:.2f}%\")\n",
    "    print(f\"Avg Training Loss: {avg_loss}\")\n",
    "\n",
    "def testing_loop(testing_dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    accuracy.reset()\n",
    "    size = len(testing_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for images,labels in testing_dataloader:\n",
    "            output = net(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            total_loss += loss\n",
    "            accuracy.update(output,labels)\n",
    "    avg_loss = total_loss / size\n",
    "    print(f\"Avg Testing Accuracy: {accuracy.compute() * 100 :.2f}%\")\n",
    "    print(f\"Avg Testing Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Avg Testing Accuracy: 6.63%\n",
      "Avg Testing Loss: 2.306647777557373\n",
      "Avg Training Accuracy: 84.10%\n",
      "Avg Training Loss: 0.5524539351463318\n",
      "------------------\n",
      "Epoch: 1\n",
      "Avg Testing Accuracy: 95.45%\n",
      "Avg Testing Loss: 0.15634384751319885\n",
      "Avg Training Accuracy: 95.89%\n",
      "Avg Training Loss: 0.13817347586154938\n",
      "------------------\n",
      "Epoch: 2\n",
      "Avg Testing Accuracy: 97.16%\n",
      "Avg Testing Loss: 0.09445755928754807\n",
      "Avg Training Accuracy: 96.98%\n",
      "Avg Training Loss: 0.09819365292787552\n",
      "------------------\n",
      "Epoch: 3\n",
      "Avg Testing Accuracy: 97.70%\n",
      "Avg Testing Loss: 0.07518746703863144\n",
      "Avg Training Accuracy: 97.60%\n",
      "Avg Training Loss: 0.07870824635028839\n",
      "------------------\n",
      "Epoch: 4\n",
      "Avg Testing Accuracy: 97.85%\n",
      "Avg Testing Loss: 0.06654796749353409\n",
      "Avg Training Accuracy: 97.95%\n",
      "Avg Training Loss: 0.06838203221559525\n",
      "------------------\n",
      "Epoch: 5\n",
      "Avg Testing Accuracy: 98.06%\n",
      "Avg Testing Loss: 0.06257737427949905\n",
      "Avg Training Accuracy: 98.09%\n",
      "Avg Training Loss: 0.06178075075149536\n",
      "------------------\n",
      "Epoch: 6\n",
      "Avg Testing Accuracy: 98.12%\n",
      "Avg Testing Loss: 0.06154683977365494\n",
      "Avg Training Accuracy: 98.25%\n",
      "Avg Training Loss: 0.057251762598752975\n",
      "------------------\n",
      "Epoch: 7\n",
      "Avg Testing Accuracy: 98.00%\n",
      "Avg Testing Loss: 0.06360787153244019\n",
      "Avg Training Accuracy: 98.37%\n",
      "Avg Training Loss: 0.053842149674892426\n",
      "------------------\n",
      "Epoch: 8\n",
      "Avg Testing Accuracy: 97.82%\n",
      "Avg Testing Loss: 0.0684233084321022\n",
      "Avg Training Accuracy: 98.48%\n",
      "Avg Training Loss: 0.050862349569797516\n",
      "------------------\n",
      "Epoch: 9\n",
      "Avg Testing Accuracy: 97.89%\n",
      "Avg Testing Loss: 0.06261414289474487\n",
      "Avg Training Accuracy: 98.54%\n",
      "Avg Training Loss: 0.04741714522242546\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Epoch: {i}\")\n",
    "    testing_loop(testing_dataloader, loss_fn)\n",
    "    training_loop(training_dataloader, optimizer, loss_fn)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "tensor([[ -4.7197,   3.3315,  -6.1920,  11.8447,  -2.9824,  -0.0817, -20.6869,\n",
      "          -5.9417,  -5.9319,   3.6082]], grad_fn=<AddmmBackward0>)\n",
      "Efter softmax:\n",
      "tensor([[6.3972e-08, 2.0072e-04, 1.4674e-08, 9.9953e-01, 3.6349e-07, 6.6108e-06,\n",
      "         7.4389e-15, 1.8849e-08, 1.9034e-08, 2.6469e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Modul bud: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afprøver modellen på et givent index af billederne.\n",
    "img_idx = 10\n",
    "pred = net(training_images[img_idx][0].unsqueeze(0)) \n",
    "print(\"Model output:\")\n",
    "print(pred)\n",
    "sm = nn.Softmax(dim=1)\n",
    "print(\"Efter softmax:\")\n",
    "print(sm(pred))\n",
    "print(f\"Modul bud: {pred.argmax()}\")\n",
    "print()\n",
    "ts.show(training_images[img_idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentSystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
