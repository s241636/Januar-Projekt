{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchshow as ts\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importere dataset, kun træning indtil videre.\n",
    "training_images = MNIST(root='data', transform=ToTensor(), train=True)\n",
    "training_dataloader = DataLoader(training_images, batch_size=1000)\n",
    "# testing_images = MNIST(root='data', transform=ToTensor(), train=False)\n",
    "# testing_dataloader = DataLoader(testing_images, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laver det Neurale Netværk, og opstiller et accuracy objekt til at måle hvor god modellen er.\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,10)\n",
    ")\n",
    "# Bruger crossentropy til at udregne losset, og indstiller optimizeren.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop'er over 10 epoker, og udregner loss'et og accuracy for hvert.\n",
    "for i in range(10):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    size = len(training_dataloader)\n",
    "    for images,labels in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_accuracy += accuracy(output, labels)\n",
    "    avg_accuracy = total_accuracy / size\n",
    "    print(f\"Avg Accuracy: {((total_accuracy / size)*100):.2f}%\")\n",
    "    print(f\"Avg Loss: {total_loss / size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "tensor([[ -4.2501, -29.3979,  -0.8760, -12.1050,  14.7614, -11.8239,   1.7023,\n",
      "          -3.3504,   1.0904,   0.9045]], grad_fn=<AddmmBackward0>)\n",
      "Efter softmax:\n",
      "tensor([[5.5386e-09, 6.6353e-20, 1.6172e-07, 2.1483e-12, 1.0000e+00, 2.8456e-12,\n",
      "         2.1306e-06, 1.3619e-08, 1.1555e-06, 9.5946e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Modul bud:\n",
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afprøver modellen på et givent index af billederne.\n",
    "img_idx = 222\n",
    "pred = net(training_images[img_idx][0])\n",
    "print(\"Model output:\")\n",
    "print(pred)\n",
    "sm = nn.Softmax(dim=1)\n",
    "print(\"Efter softmax:\")\n",
    "print(sm(pred))\n",
    "print(\"Modul bud:\")\n",
    "print(pred.argmax())\n",
    "ts.show(training_images[img_idx][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
