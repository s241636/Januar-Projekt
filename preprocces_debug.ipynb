{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchshow as ts\n",
    "import ImageProcessing as ip\n",
    "import os\n",
    "import cv2 as cv \n",
    "import mnist_cnn\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = mnist_cnn.cnn()\n",
    "net.load_state_dict(torch.load('/Users/sif_wittus/Documents/GitHub/Januar-Projekt/trained_cnn.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_folder):\n",
    "    image_paths = os.listdir(image_folder)\n",
    "    if \".DS_Store\" in image_paths:\n",
    "        image_paths.remove(\".DS_Store\")\n",
    "    image_paths = [f\"{image_folder}/{image}\" for image in image_paths]\n",
    "    images = [torchvision.io.decode_image(path) for path in image_paths]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_with_labels(image_folder):\n",
    "    image_paths = os.listdir(image_folder)\n",
    "    if \".DS_Store\" in image_paths:\n",
    "        image_paths.remove(\".DS_Store\")\n",
    "    image_paths = [f\"{image_folder}/{image}\" for image in image_paths]\n",
    "    images = [torchvision.io.decode_image(path) for path in image_paths]\n",
    "    labels = []\n",
    "    for path in image_paths:\n",
    "        _, label = path.split('(')\n",
    "        label = int(label[0])\n",
    "        labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dida_images_with_labels(dida_folder):\n",
    "    image_paths = os.listdir(dida_folder)\n",
    "    if \".DS_Store\" in image_paths:\n",
    "        image_paths.remove(\".DS_Store\")\n",
    "    labels = []\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        label = path[0]\n",
    "        labels.append(int(label))\n",
    "        image = torchvision.io.decode_image(f\"DIDA/{path}\")\n",
    "        images.append(image)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "image::decode_image() Expected a value of type 'Tensor' for argument 'data' but instead found type 'str'.\nPosition: 0\nValue: 'preprocess_testset/image25 (5).png'\nDeclaration: image::decode_image(Tensor data, int mode, bool apply_exif_orientation=False) -> Tensor\nCast error details: Unable to cast preprocess_testset/image25 (5).png to Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tester preprocess v1 accuracy på alle testset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess_testset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m load_images_with_labels(image_folder)\n\u001b[1;32m      4\u001b[0m image_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[1;32m      5\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(images,labels)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mload_images_with_labels\u001b[0;34m(image_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m     image_paths\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.DS_Store\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[0;32m----> 6\u001b[0m images \u001b[38;5;241m=\u001b[39m [torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mdecode_image(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/IntelligentSystems/lib/python3.12/site-packages/torchvision/io/image.py:269\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    268\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 269\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_image(\u001b[38;5;28minput\u001b[39m, mode\u001b[38;5;241m.\u001b[39mvalue, apply_exif_orientation)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/IntelligentSystems/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: image::decode_image() Expected a value of type 'Tensor' for argument 'data' but instead found type 'str'.\nPosition: 0\nValue: 'preprocess_testset/image25 (5).png'\nDeclaration: image::decode_image(Tensor data, int mode, bool apply_exif_orientation=False) -> Tensor\nCast error details: Unable to cast preprocess_testset/image25 (5).png to Tensor"
     ]
    }
   ],
   "source": [
    "# Tester preprocess v1 accuracy på alle testset\n",
    "image_folder = \"preprocess_testset\"\n",
    "images, labels = load_images_with_labels(image_folder)\n",
    "image_count = len(images)\n",
    "test_set = zip(images,labels)\n",
    "correct_predictions = 0\n",
    "for idx, (image, label) in enumerate(test_set):\n",
    "    image = ip.preprocess_stack_v1(image)\n",
    "    # ts.save(image, f'stack1_images/{idx}.jpg')\n",
    "    pred = net(image).argmax().item()\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "acc = (correct_predictions / image_count) * 100\n",
    "print(f\"V1 Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester preprocess v2 accuracy på eget testset\n",
    "import ImageProcessing\n",
    "importlib.reload(ImageProcessing) \n",
    "# Tester preprocess v2 på alle billeder\n",
    "image_folder = \"preprocess_testset\"\n",
    "images, labels = load_images_with_labels(image_folder)\n",
    "image_count = len(images)\n",
    "test_set = zip(images,labels)\n",
    "correct_predictions = 0\n",
    "for idx, (image, label) in enumerate(test_set):\n",
    "    image = ip.preprocess_stack_v2(image)\n",
    "    # ts.save(image, f'stack2_images/{idx}.jpg')\n",
    "    pred = net(image).argmax().item()\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "acc = (correct_predictions / image_count) * 100\n",
    "print(f\"V2 Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ImageProcessing\n",
    "importlib.reload(ImageProcessing) \n",
    "# Tester preprocess v1 accuracy på DIDA testset\n",
    "images, labels = load_dida_images_with_labels(\"DIDA\")\n",
    "image_count = len(images)\n",
    "test_set = zip(images,labels)\n",
    "correct_predictions = 0\n",
    "for idx, (image, label) in enumerate(test_set):\n",
    "    image = ip.preprocess_stack_v1(image)\n",
    "    # ts.save(image, f'stack1_images/{idx}.jpg')\n",
    "    pred = net(image).argmax().item()\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "acc = (correct_predictions / image_count) * 100\n",
    "print(f\"V1 Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ImageProcessing\n",
    "importlib.reload(ImageProcessing) \n",
    "# Tester preprocess v2 accuracy på DIDA testset\n",
    "images, labels = load_dida_images_with_labels(\"DIDA\")\n",
    "image_count = len(images)\n",
    "test_set = zip(images,labels)\n",
    "correct_predictions = 0\n",
    "for idx, (image, label) in enumerate(test_set):\n",
    "    image = ip.preprocess_stack_v2(image)\n",
    "    # ts.save(image, f'stack1_images/{idx}.jpg')\n",
    "    pred = net(image).argmax().item()\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "acc = (correct_predictions / image_count) * 100\n",
    "print(f\"V1 Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image testing\n",
    "import ImageProcessing\n",
    "importlib.reload(ImageProcessing) \n",
    "image_folder = \"DIDA/0\"\n",
    "images = load_images(image_folder)\n",
    "image = images[10]\n",
    "image = ip.preprocess_stack_v2(image)\n",
    "ts.show(image)\n",
    "print(net(image).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viser billeder modellen stadig fejler på\n",
    "import ImageProcessing\n",
    "importlib.reload(ImageProcessing) \n",
    "# Tester preprocess v2 på alle billeder\n",
    "image_folder = \"preprocess_testset\"\n",
    "images, labels = load_images_with_labels(image_folder)\n",
    "image_count = len(images)\n",
    "test_set = zip(images,labels)\n",
    "for image, label in test_set:\n",
    "    image = ip.preprocess_stack_v2(image)\n",
    "    pred = net(image).argmax().item()\n",
    "    if pred != label:\n",
    "        ts.show(image)\n",
    "        print(sm(net(image)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentSystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
